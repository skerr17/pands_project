{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b267c5",
   "metadata": {},
   "source": [
    "\n",
    "# **Iris Dataset Analysis**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Authored by: Stephen Kerr\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2467e",
   "metadata": {},
   "source": [
    "\n",
    "## **Iris Dataset Introduction**\n",
    "\n",
    "The **Iris Dataset** is a famous classix multi-class classification dataset, which contains **$150$ samples** of Iris flowers from three species: **Setosa**, **Versicolor**, and **Virginica**.   \n",
    "Each sample includes four features: **Sepal length** (in cm), **Sepal width** (in cm), **Petal length** (in cm), **Petal width** (in cm)\n",
    "\n",
    "The raw data can be seen in **Inputs** folder in [iris.data](https://github.com/skerr17/pands_project/blob/main/inputs/iris.data) which was sourced from [UCI Machine Learning Repository - Iris Dataset](https://archive.ics.uci.edu/dataset/53/iris). \n",
    "\n",
    "The Image below illustrates the Three Iris Flower Species and their anatomy (Image sourced from [here](https://www.analyticsvidhya.com/blog/2022/06/iris-flowers-classification-using-machine-learning/)).\n",
    "\n",
    "![iris flower image](iris_species_image.png) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b749716",
   "metadata": {},
   "source": [
    "\n",
    "## **Exploring the Iris Dataset**\n",
    "\n",
    "Following doing some research on the Iris Dataset I conducted some initial exploration of the data in the [iris.data](https://github.com/skerr17/pands_project/blob/main/inputs/iris.data). \n",
    "\n",
    "The function `generate_descriptive_statistics()` found in [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py), generates the descriptive statistics for the global Iris Dataset and by species (`global_descriptive_stats` and `stats_by_species`).  \n",
    "\n",
    "`generate_descriptive_statistics()` calculates the Count, Mean, Standard Deviation, Minimum, $25th$ Percentile, Median, $75th$ Percentile, and Maximum for all $n=150$ Iris samples. \n",
    "\n",
    "Whereas, `stats_by_species` calculates the Count, Mean, Standard Deviation, Minimum, $25th$ Percentile, Median, $75th$ Percentile, and Maximum for each **Species** of Iris. Using the `groupby('Species')` to subset the dataset based upon each Samples species (**Setosa**, **Versicolor**, and **Virginica**) each having a sample size of $n=50$.\n",
    "\n",
    "Finally, saving all the Descriptive Statistics in a Tabular format in a `Text` file titled **[iris_descriptive_stats.txt](outputs/iris_descriptive_stats.txt)** in the **Outputs Folder**.\n",
    "\n",
    "### Insights / Observations:\n",
    "\n",
    "#### Key insights from the **Global Descriptive Statistics** are the following:\n",
    "\n",
    "-  The **Petal Length & Width** are more variable than the **Sepal Length & Width**, suggesting they might have more predictive power for classification (will return to this point to validate).\n",
    "- **Sepal Width** is the most consistent feature across all the species with a standard deviation of $0.43 cm$.\n",
    "- **Petal Length** has the highest variability with a standard deviation of $1.77 cm$.  \n",
    "\n",
    "Overall, the **global statistics** show that Sepal dimensions tend to be larger than Petal dimensions. Specifically, **Sepal Length** has a mean of $5.84 cm$ and **Sepal Width** a mean of $3.05 cm$, compared to **Petal Length** with a mean of $3.76 cm$ and **Petal Width** with $1.20 cm$.\n",
    "\n",
    "#### Key insights from the **Descriptive Statistics per Species** are the following:\n",
    "\n",
    "- **Setosa** is the smallest in overall measurements and has low variability. Making it the easiest to classify from the others.\n",
    "- **Versicolor** is the intermediate in terms of feature values with more variability than **Setosa** and commonly overlapping with **Virginica** which means it is harder to classify.\n",
    "- **Virginica** is the largest Sepal and Petal dimensions with the highest within species variation.\n",
    "\n",
    "As suggested in the **Global Descriptive Statistics** the **Petal Length & Width** is the most effective feature for distinguishing species, while **Sepal Width** being the hardest to classify the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4100f383",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_descriptive_statistics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate descriptive statistics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m global_descriptive_stats, stats_by_species = \u001b[43mgenerate_descriptive_statistics\u001b[49m(\n\u001b[32m      3\u001b[39m     iris_data, output_dir, variables_titles, species, format_species\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Display global descriptive statistics\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGlobal Descriptive Statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_descriptive_statistics' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate descriptive statistics\n",
    "global_descriptive_stats, stats_by_species = generate_descriptive_statistics(\n",
    "    iris_data, output_dir, variables_titles, species, format_species\n",
    ")\n",
    "\n",
    "# Display global descriptive statistics\n",
    "print(\"Global Descriptive Statistics:\")\n",
    "display(global_descriptive_stats)\n",
    "\n",
    "# Display descriptive statistics by species\n",
    "print(\"Descriptive Statistics by Species:\")\n",
    "display(stats_by_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5ba90",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Data Visualisation**\n",
    "\n",
    "After conducting the descriptive analysis I went about visulasing the Iris dataset Features and their relationships. \n",
    "- First with **Histograms** for each feature using **Matplotlib's** `hist()` method (see documentation [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)).\n",
    "- **Scatter Plots** for each feature using **Matplotlib's** `scatter()` method (see documentation [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html)).\n",
    "- **Pairs Plot** for each feature and their relationship with eachother using **Seaborn's** `pairplot()` function (see documentation [here](https://seaborn.pydata.org/generated/seaborn.pairplot.html)),\n",
    "- Finally, a **Heatmap** of the of the  **Pearson Correlation Coefficient** calculated using **`.corr()`** a method from **Pandas** (see `.corr()` method documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)) and visulaised using **Seaborn's** `heatmap()` function (see documentation [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba375228",
   "metadata": {},
   "source": [
    "\n",
    "### **Histograms of the Features**\n",
    "\n",
    "Histograms are used to display the distribution of numerical features within a dataset by showing the frequency of observations within defined intervals or 'bins'. \n",
    "\n",
    "The function `plot_histograms()` found in [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py), generates a Histogram for each variable with the different species colour coded to individual `.png` files titled `{variable}_histogram.png` in the **[Outputs Folder](https://github.com/skerr17/pands_project/tree/main/outputs)**.\n",
    "\n",
    "See all the Histograms linked here:\n",
    "- [petal_length_histogram.png](https://github.com/skerr17/pands_project/blob/main/outputs/petal_length_histogram.png)\n",
    "- [petal_width_histogram.png](https://github.com/skerr17/pands_project/blob/main/outputs/petal_width_histogram.png)\n",
    "- [sepal_length_histogram.png](https://github.com/skerr17/pands_project/blob/main/outputs/sepal_length_histogram.png)\n",
    "- [sepal_width_histogram.png](https://github.com/skerr17/pands_project/blob/main/outputs/sepal_width_histogram.png)\n",
    "\n",
    "#### Insights / Observations:\n",
    " \n",
    "For the Iris dataset, plotting histograms for each feature helps reveal the spread of the data across each species. For example, the [petal_length_histogram.png](https://github.com/skerr17/pands_project/blob/main/outputs/petal_length_histogram.png) clearly shows the difference between the species with the **Setosa** distinctly separated from **Versicolor** and **Virginica**. Therefore, for classification the **Petal Length** is useful feature. Note, see the below Histogram.\n",
    "\n",
    "<img src=\"outputs/petal_length_histogram.png\" alt=\"Petal Length Histogram\" style=\"width:400px; height:auto;\">\n",
    "\n",
    "\n",
    "\n",
    "Histograms also aid in spotting skewness or outliers that could influence modeling decisions. The Iris Dataset doesn't seem to have any significant outliers. \n",
    "\n",
    "Notably, the [sepal_width_histogram.png](https://github.com/skerr17/pands_project/blob/main/outputs/sepal_width_histogram.png) being less effective is visualised by the overlap between the species. See the below Histogram.  /workspaces/pands_project/outputs/sepal_width_histogram.png\n",
    "\n",
    "<img src=\"outputs/sepal_width_histogram.png\" alt=\"Sepal Width Histogram\" style=\"width:400px; height:auto;\">\n",
    "\n",
    "Reference: see this stack overflow thread to understand how I resized my Histograms linked [here](https://stackoverflow.com/questions/41598916/resize-the-image-in-jupyter-notebook-using-markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69800075",
   "metadata": {},
   "source": [
    "\n",
    "### **Scatter Plot of the Features**\n",
    "\n",
    "Scatter plots are used to visualise the relationship between two numerical features.\n",
    "\n",
    "The function `plot_scatter()` found in [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py), generates a Scatter Plot for each variable with the different species colour coded to a single `.png` files titled [iris_scatter.png](https://github.com/skerr17/pands_project/blob/main/outputs/iris_scatter.png) in the **[Outputs Folder](https://github.com/skerr17/pands_project/tree/main/outputs)**. \n",
    "\n",
    "#### Insights / Observations: \n",
    "\n",
    "- There appears to be a strong correlation between **Petal Lenght** and **Petal Width** within all the Iris Species.\n",
    "- As was seen in the Descriptive Statistics and the Histograms **Setosa** is the most distinct from the two other species **Versicolor** and **Virginica** that share significant overlap. \n",
    "\n",
    "See the Scatter Plots below.\n",
    "\n",
    "<img src=\"outputs/iris_scatter.png\" alt=\"Iris Scatter Plots\" style=\"width:700px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e52880",
   "metadata": {},
   "source": [
    "\n",
    "### **Pairs Plot of the Features**\n",
    "\n",
    "**Seaborn's** `pairplot()` function (see documentation [here](https://seaborn.pydata.org/generated/seaborn.pairplot.html)) is a powerful tool for exploratory data analysis, as it provides a quick and comprehensive visual summary of the relationships between multiple numerical variables. See [Geek For Geeks]('https://www.geeksforgeeks.org/python-seaborn-pairplot-method/') for a helpful guide on how to use `pairplot()`.\n",
    "\n",
    "The function `pairsplots()` found in [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py), generates a a grid of plots for each variable with the different species colour coded to a single `.png` files titled [iiris_pairplot.png](https://github.com/skerr17/pands_project/blob/main/outputs/iris_pairplot.png) in the **[Outputs Folder](https://github.com/skerr17/pands_project/tree/main/outputs)**. \n",
    "\n",
    "In [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py) `pairsplots()` was configured with the following:\n",
    "- **Off-Diagonal plots** show scatterplots with regression lines (using the argument `kind='reg'` ), helping us understand the correlation and trends. \n",
    "- **Diagonal plots** display the distribution of each variable giving us insight into the spread of the data across the Iris dataset. \n",
    "- The `hue='species'` argument and the `palette='colours'` argument colours the data points based upon the species with the colour coding used throughout this analysis of the Iris Dataset: `{'setosa': 'red', 'versicolor': 'green', 'virginica': 'blue'}`\n",
    "\n",
    "#### Insights / Observations: \n",
    "\n",
    "The **Pairs Plot** again validates the observations seen in the other visualisations, for instance, the **Diagonal plots** again highlight the previously stated difference between **Sepal** dimensions and **Petal** dimensions in terms of their ability to differentiate between species. \n",
    "\n",
    "See the Pairs Plot below.\n",
    "\n",
    "<img src=\"outputs/iris_pairplot.png\" alt=\"Iris Scatter Plots\" style=\"width:700px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ccd1b",
   "metadata": {},
   "source": [
    "\n",
    "### **Heatmap of the Correlation Coefficient**\n",
    "\n",
    "The **Pearson Correlation Coefficient** (also known as the Standard Correlation Coefficient) ranges from -1 to 1 and measures the linear relationship between two variables. Heatmaps is another way to easily visualise representations of correlation between two variables.  \n",
    "\n",
    "The function `corrleation_matrix_heatmap()` found in [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py), generates the Correlation Matrix of the Iris Dataset (in the code called `corr_matrix`) using **`.corr()`** a method from **Pandas** (see the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)).  \n",
    "Then using **Seaborn's** `heatmap()` function (see documentation [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html)) a Heatmap is created with a with a Colour Bar included to explaining what the Colours represent in the heatmap: \n",
    "- **Red** indicates a Strong Positive Correlation between Features (values close to **1**), \n",
    "- **Light Grey** indicates a Weak Correlation between the Features (values close to **0**),\n",
    "- **Blue** indicates a Strong Negative Correlation between the Features (values close to **-1**).\n",
    "\n",
    "Finally the Heatmap is saved as a `.png` file titled **[iris_correlation_matrix.png](https://github.com/skerr17/pands_project/blob/main/outputs/iris_correlation_matrix.png)** in the **[Outputs Folder](https://github.com/skerr17/pands_project/tree/main/outputs)**.\n",
    "\n",
    "\n",
    "#### Insights / Observations:\n",
    "The Correlation Coefficient is useful as it helps identify relationships between features, but it is important to note that potetnially strong correlations may provide redundant information, and weak correlations may be indicate independent features, but also there could be an unknown confounding variable. \n",
    "\n",
    ">***'Correlation Does Not Imply Causation'***\n",
    "\n",
    "An **absolute value of 1** (i.e., 1 or -1) indicates a perfect linear relationship between the x and y, with every data point lying on the line. In the Heatmap a value of 1 is seen when a **Feature** is correlated with itself. This value of 1 doesn't provide any useful insight and is an artifact of the way the calculation was done and displayed.\n",
    "\n",
    "In the Heatmap an example of a **Strong Positive Correlation Coefficient** is seen between **Sepal Length** and **Petal Length** with a value of $0.87$, indicated visually with a close to red colour. A **Positive Correlation Coefficient** means when x increases, y tends to increase as well or when **Sepal Length** increases so **Petal Length** tends to increase with a Strong Correlation but not perfect. \n",
    "> **Note:** The **Strongest Positive Correlation** is between ***'Petal legth'*** and ***'Petal Width'*** with a value of $0.96$ which is indicated in the Heatmap with red.  \n",
    "\n",
    "A value of 0 implies there is no linear relationship between the variables. Therefore values with close to 0 have a **Weak Correlation Coefficient**. In the Heatmap for example, **Sepal Length** and **Sepal Width** have a **Weak Negative Correlation Coefficient** of $-0.11$ indicated by a light grey colour.  \n",
    "\n",
    "A **Negative Correlation Coefficient** means as x increases y tends to decreases. For example, **Petal Length** and **Sepal Width** have a **Weak to Moderate Negative Correlation Coefficient** of $-0.42$ which suggests that when **Petal Length** increases **Sepal Width** typically decreases but not in a perfectly linear way. \n",
    "\n",
    "See the Heatmap below.\n",
    "\n",
    "\n",
    "<img src=\"outputs/iris_correlation_matrix.png\" alt=\"Iris Correlation Matrix Heatmap\" style=\"width:700px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf73636",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Principal Component Analysis** \n",
    "\n",
    "\n",
    "Following on from the visualisation of the features. I decided to perform a **Principal Component Analysis (PCA)** on the Iris Dataset. Having too many features in data can cause problem like overfitting (a model performs well on training data but fails on new data), slower computation, and lower accuracy. Coined **the curse of dimensionality** (see [geeks for geeks article on curse of dimensionality](https://www.geeksforgeeks.org/curse-of-dimensionality-in-machine-learning/) and [wiki](https://en.wikipedia.org/wiki/Curse_of_dimensionality) for more details on the curse of dimensionality). \n",
    "\n",
    "PCA is one of the most widely used dimensionality reduction technique. It works by transforming high-dimensional data into a lower-dimensional space while maximizing the variance (spread) of the data, preserving the most important patterns and relationships in the data. PCA prioritises the directions where the data varies the most as more variation = more useful information. (see [geek for geek article on PCA](https://www.geeksforgeeks.org/principal-component-analysis-pca/)).\n",
    "\n",
    "\n",
    "The function `pca_analysis()` found in [analysis_code.py](https://github.com/skerr17/pands_project/blob/main/analysis_code.py), perfoms PCA on the Iris Dataset to reduce the number of features from 4 to 2 and then visualises it into a scatter plot. \n",
    "\n",
    "\n",
    ">Note, the reason I am doing PCA is as part of my job I work with Data Scientist in the Life Sciences Industry that create Models of Real-Time Data for Manufacturing Process (for example the various conditions measurements (pH, live cell count, temperature etc.,) bioreactors producing drugs). They have mentioned things in my work like PCA and MSPM (multivariate statistical process monitoring). They have to do feature engineering a good bit, so I wanted to learn what PCA means.\n",
    "\n",
    "\n",
    "#### **Preprocessing for PCA**\n",
    "1. The Iris dataset is first standardised the Iris Dataset using from **sklearn.preprocessing** `StandardScaler()` (see documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). The scaler normalises the Iris Dataset features so that their mean is zero and their variance is one. \n",
    "2. The `scaled_data` (which will be used in the PCA) is created using `fit_transform()` this method from **sklearn** fits the scaled data to the Iris Dataset and then transforms it (see documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn.base.TransformerMixin.fit_transform)). \n",
    "\n",
    "#### **Performing the PCA** \n",
    "Now that we have the Iris Dataset standardised we can perform the PCA using `PCA()` function from **sklearn.decomposition** (see the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). In the function we `PCA()` can define the number of principal components we want to reduce the Iris Dataset to. In my code I decided upon reducing the data to 2 (`PCA(n_components=2)`). Similarliy to before we need to now apply the PCA to the scaled data using the `fit_transform()` method again, this time however it will return the data with a reduced dimensionality (originally 4D, but now will be 2D).\n",
    "\n",
    "#### **Package the PCA Results into a DataFrame** \n",
    "The `pca_result` variable that contains the PCA Results is a **numpy array** to make it easier to plot I packaged the PCA Results into a Pandas DataFrame called `pca_df`. Setting the column names (`columns=['PC1', 'PC2']`) and adding in the **Species** column to enable plotting the of the PCA Results by their Species. \n",
    "\n",
    "#### **Calculated the Explained Variance Ratio for the PCA** \n",
    "In PCA the **explained variance ratio** indicates how much of the original data's variability is captured by each principal component, i.e., the information that was retained from the original data after the PCA was performed. Using **sklearn's** `explained_variance_ratio_` (see the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). \n",
    "\n",
    "Explained Variance Ratios: \n",
    "- `pc1_variance`$= 72.77\\%$,\n",
    "- `pc2_variance`$= 23.03\\%$,\n",
    "- `total_variance`$=95.80\\%$ (sum of both principal components variance). \n",
    "\n",
    "\n",
    "#### **Plotting the PCA Results into a Scatter Plot** \n",
    "The final thing the `pca_analysis()` function does is it plots the **PCA Result Data** into a scatter plot using **Mathplotlib** and saves as a `.png` file titled [iris_pca.png](https://github.com/skerr17/pands_project/blob/main/outputs/iris_pca.png) in the **[Outputs Folder](https://github.com/skerr17/pands_project/tree/main/outputs)**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Insights / Observations:**\n",
    "\n",
    "The **PCA** on the Iris Dataset successfully reduced the dimensionality from 4D to 2D while preserving most of the variance, with the clear separation of **Setosa** from the other species, and highlighted the overlap between **Versicolor** and **Virginica**.\n",
    "\n",
    "See the PCA Scatter Plot below.\n",
    "\n",
    "<img src=\"outputs/iris_pca.png\" alt=\"Iris PCA Plot\" style=\"width:700px; height:auto;\">\n",
    "\n",
    "\n",
    "To highlight the power of PCA let us do visual examination of the [iris_scatter.png](https://github.com/skerr17/pands_project/blob/main/outputs/iris_scatter.png) created previously with the PCA scatter plot [iris_pca.png](https://github.com/skerr17/pands_project/blob/main/outputs/iris_pca.png). Noted added the two images into a table using HTML so they can be viewed side by side (see Stack Overflow thread [here](https://stackoverflow.com/questions/33647774/how-to-include-two-pictures-side-by-side-in-for-ipython-notebook-jupyter)). \n",
    "\n",
    "As can be seen below in the two images, the **iris_scatter.png** (on the right) shows all the linear relationships between the features of the Iris Dataset (in total 6 scatter plots), some show a strong correlation such as **Petal Lenght vs Petal Width** while others show a weak correlation like **Sepal Width vs Petal Width** as discussed previously. Whereas the **iris_pca.png** (on the left) by combining the features into two principal components simplifies the data into 1 scatter plot, with minimal information loss but reducing the visual overload. \n",
    "\n",
    "The Separation of the different Iris Species was also maintained following the PCA reduction of features from 4 to 2. The distinct separation of **Setosa** described previously and the overlap of **Versicolor** and **Virginica** was  maintained. \n",
    "\n",
    "Due to the above observations it is fair to say the PCA conducted was an efficient summariser of the original data structure reducing complexity but maintaining the information (with $95.80\\%$ total explained variance). \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"outputs/iris_pca.png\" alt=\"Iris PCA Plot\" style=\"width:700px; height:auto;\"></td>\n",
    "        <td><img src=\"outputs/iris_scatter.png\" alt=\"Iris Scatter Plots\" style=\"width:700px; height:auto;\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458806e9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Conclusion** \n",
    "\n",
    "This Jupyter Notebook provides a comprehensive analysis of the Iris Dataset, showcasing various data exploration, visualisation, and finally a dimensionality reduction technique (PCA). Below is a summary of the steps we took and key insights gained: \n",
    "\n",
    "#### 1.  **Iris Dataset Introduction**\n",
    "\n",
    "-   Following some external research we outlined the context of the Iris Dataset, it's origin, it's use as multi-classification dataset, and the number of samples and what they represent Iris flowers in three different species.\n",
    "\n",
    "#### 2.  **Exploring Iris Dataset**\n",
    "\n",
    "-   After we established the context around the data we took a deepdive into exploring it by conducting descriptive statistics both globally and by species. \n",
    "-   This initial exploration provided the foundation for futher analysis with key observations being **Petal Length** and **Petal Width** showing high variability, making them more useful for classification. Also **Sepal Width** was the most consistent feature across the species highlighting it as not being good for classification. \n",
    "\n",
    "#### 3.  **Data Visualisation**\n",
    "\n",
    "- Next we conducted some data visualisation creating: \n",
    "\n",
    "    - **Histograms** that highlighted the distribution of each feature across the species, \n",
    "    - **Scatter Plots** displayed the relationship between features, with again strong correlations observed between **Petal Length** and **Petal Width**,\n",
    "    - **Pairs Plot** provided a comprehensive view of all the feature relationships and the distribution of each feature (combining the value of the histograms and scatter plots into one grid), \n",
    "    - **Heatmap** visualised the correlation matrix, this confirmed the strong positive correlation between **Petal Length** and **Petal Width**. \n",
    "\n",
    "\n",
    "#### 4.  **Principal Component Analysis (PCA)**\n",
    "\n",
    "- Finally, a PCA was performed to reduce the dataset's dimensionality from 4 to 2 while retaining $95.80\\%$ of the toal varaince seen in the orginal dataset. \n",
    "\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- The Iris Dataset's features, particualrly **Petal Length** and **Petal Width** , are effective for distingushing Iris species, especially the **Setosa** from the other species. \n",
    "\n",
    "- However, as shown above the the overlap between **Versicolor** and **Virginica**. Suggesting these two species are more difficult to differentiate based on the features provided in the Iris Dataset.  \n",
    "\n",
    "- PCA proved to be a valuable tool for reducing the complexity while preserving the Iris Dataset structure, enabling better visualisation and understanding of the data. \n",
    "\n",
    "Overall, the analysis demonstrated the importance of combining statistical exploration, visualisation, and dimensionality reduction for effective data understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76165d59",
   "metadata": {},
   "source": [
    "# **End** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
